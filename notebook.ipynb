{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries and data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/mwitiderrick/kerasDO/master/HR_comma_sep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>department</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years department  \\\n",
       "0                   3              0     1                      0      sales   \n",
       "1                   6              0     1                      0      sales   \n",
       "2                   4              0     1                      0      sales   \n",
       "3                   5              0     1                      0      sales   \n",
       "4                   3              0     1                      0      sales   \n",
       "\n",
       "   salary  \n",
       "0     low  \n",
       "1  medium  \n",
       "2  medium  \n",
       "3     low  \n",
       "4     low  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## You can get a glimpse at the dataset you’re working with by using head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll now proceed to convert the categorical columns to numbers. You do this by converting them to dummy variables. Dummy variables are usually ones and zeros that indicate the presence or absence of a categorical feature. In this kind of situation, you also avoid the dummy variable trap by dropping the first dummy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['department','salary']\n",
    "df_final = pd.get_dummies(df,columns=feats,drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feats = ['department','salary'] defines the two columns for which you want to create dummy variables. pd.get_dummies(df,columns=feats,drop_first=True) will generate the numerical variables that your employee retention model requires. It does this by converting the feats that you define from categorical to numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve loaded in the dataset and converted the salary and department columns into a format the keras deep learning model can accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separating Your Training and Testing Datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_final.drop(['left'],axis=1).values\n",
    "y = df_final['left'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our deep learning model expects to get the data as arrays. Therefore we use numpy to convert the data to numpy arrays with the .values attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll use 70% of the data for training and 30% for testing. The training ratio is more than the testing ratio because you’ll need to use most of the data for the training process. If desired, you can also experiment with a ratio of 80% for the training set and 20% for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building deep learning models it is usually good practice to scale our dataset in order to make the computations more efficient. In this step, we’ll scale the data using the StandardScaler; this will ensure that our dataset values have a mean of zero and a unit variable. This transforms the dataset to be normally distributed. We’ll use the scikit-learn StandardScaler to scale the features to be within the same range. This will transform the values to have a mean of 0 and a standard deviation of 1. This step is important because we’re comparing features that have different measurements; so it is typically required in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have scaled all your dataset features to be within the same range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use keras to build the deep learning model. To do this, we’ll import keras, which will use tensorflow as the backend by default. From keras, we’ll then import the Sequential module to initialize the artificial neural network. An artificial neural network is a computational model that is built using inspiration from the workings of the human brain. We’ll import the Dense module as well, which will add layers to our deep learning model.\n",
    "\n",
    "When building a deep learning model we usually specify three layer types:\n",
    "\n",
    "- The input layer is the layer to which we’ll pass the features of our dataset. There is no computation that occurs in this layer. It serves to pass features to the hidden layers.\n",
    "- The hidden layers are usually the layers between the input layer and the output layer—and there can be more than one. These layers perform the computations and pass the information to the output layer.\n",
    "- The output layer represents the layer of our neural network that will give us the results after training our model. It is responsible for producing the output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use Sequential to initialize a linear stack of layers. Since this is a classification problem, we’ll create a classifier variable. A classification problem is a task where we have labeled data and would like to make some predictions based on the labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(9, kernel_initializer = \"uniform\",activation = \"relu\", input_dim=18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add layers using the .add() function on our classifier and specify some parameters:\n",
    "\n",
    "- The first parameter is the number of nodes that our network should have. The connection between different nodes is what forms the neural network. One of the strategies to determine the number of nodes is to take the average of the nodes in the input layer and the output layer.\n",
    "\n",
    "- The second parameter is the kernel_initializer. When we fit our deep learning model the weights will be initialized to numbers close to zero, but not zero. To achieve this we use the uniform distribution initializer. kernel_initializer is the function that initializes the weights.\n",
    "\n",
    "- The third parameter is the activation function. Our deep learning model will learn through this function. There are usually linear and non-linear activation functions. We use the relu activation function because it generalizes well on our data. Linear functions are not good for problems like these because they form a straight line.\n",
    "\n",
    "- The last parameter is input_dim, which represents the number of features in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(1, kernel_initializer = \"uniform\",activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer takes the following parameters:\n",
    "\n",
    "- The number of output nodes. We expect to get one output: if an employee leaves the company. Therefore we specify one output node.\n",
    "\n",
    "- For kernel_initializer we use the sigmoid activation function so that we can get the probability that an employee will leave. In the event that we were dealing with more than two categories, we would use the softmax activation function, which is a variant of the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll apply a gradient descent to the neural network. This is an optimization strategy that works to reduce errors during the training process. Gradient descent is how randomly assigned weights in a neural network are adjusted by reducing the cost function, which is a measure of how well a neural network performs based on the output expected from it.\n",
    "\n",
    "The aim of a gradient descent is to get the point where the error is at its least. This is done by finding where the cost function is at its minimum, which is referred to as a local minimum. In gradient descent, we differentiate to find the slope at a specific point and find out if the slope is negative or positive—you’re descending into the minimum of the cost function. There are several types of optimization strategies, but we’ll use a popular one known as adam in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer= \"adam\",loss = \"binary_crossentropy\",metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying gradient descent is done via the compile function that takes the following parameters:\n",
    "\n",
    "- optimizer is the gradient descent.\n",
    "- loss is a function that we’ll use in the gradient descent. Since this is a binary classification problem we use the binary_crossentropy loss function.\n",
    "- The last parameter is the metric that we’ll use to evaluate our model. In this case, we’d like to evaluate it based on its accuracy when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050/1050 [==============================] - 1s 927us/step - loss: 0.4382 - accuracy: 0.8188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d821e473a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re ready to fit our classifier to our dataset. Keras makes this possible via the .fit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050/1050 [==============================] - 1s 979us/step - loss: 0.2417 - accuracy: 0.9196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d8231243d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050/1050 [==============================] - 1s 979us/step - loss: 0.1961 - accuracy: 0.9391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d82314a230>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050/1050 [==============================] - 1s 866us/step - loss: 0.1779 - accuracy: 0.9471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d82315e2f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .fit() method takes a couple of parameters:\n",
    "\n",
    "- The first parameter is the training set with the features.\n",
    "\n",
    "- The second parameter is the column that we’re making the predictions on.\n",
    "\n",
    "- The batch_size represents the number of samples that will go through the neural network at each training round.\n",
    "\n",
    "- epochs represents the number of times that the dataset will be passed via the neural network. The more epochs the longer it will take to run our model, which also gives us better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve created our deep learning model, compiled it, and fitted it to our dataset. You’re ready to make some predictions using the deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Predictions on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we’ve already trained the classifier with the training set, this code will use the learning from the training process to make predictions on the test set. This will give us the probabilities of an employee leaving. We’ll work with a probability of 50% and above to indicate a high chance of the employee leaving the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting threshold\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve created predictions using the predict method and set the threshold for determining if an employee is likely to leave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a confusion matrix to check the number of correct and incorrect predictions. A confusion matrix, also known as an error matrix, is a square matrix that reports the number of true positives(tp), false positives(fp), true negatives(tn), and false negatives(fn) of a classifier.\n",
    "\n",
    "- A true positive is an outcome where the model correctly predicts the positive class (also known as sensitivity or recall).\n",
    "- A true negative is an outcome where the model correctly predicts the negative class.\n",
    "- A false positive is an outcome where the model incorrectly predicts the positive class.\n",
    "- A false negative is an outcome where the model incorrectly predicts the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3321,   95],\n",
       "       [ 142,  942]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix output means that our deep learning model made 3321 + 942 correct predictions and 95 + 142 wrong predictions. We can calculate the accuracy with: (3321 + 942) / 4500. The total number of observations in your dataset is 4500. This gives you an accuracy of 94.7%. This is a very good accuracy rate since we can achieve at least 94% correct predictions from our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a Single Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll make a single prediction given the details of one employee with our model. We will achieve this by predicting the probability of a single employee leaving the company. We’ll pass this employee’s features to the predict method. As we did earlier, we’ll scale the features as well and convert them to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred = classifier.predict(sc.transform(np.array([[0.26,0.7 ,3., 238., 6., 0.,0.,0.,0., 0.,0.,0.,0.,0.,1.,0., 0.,1.]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features represent the features of a single employee. As shown in the dataset in step 1, these features represent: satisfaction level, last evaluation, number of projects, and so on. As we did in step 3, we have to transform the features in a manner that the deep learning model can accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Add a threshold of 50% with the following code:\n",
    "\n",
    "new_pred = (new_pred > 0.5)\n",
    "new_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This threshold indicates that where the probability is above 50% an employee will leave the company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in our output that the employee won’t leave the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## You might decide to set a lower or higher threshold for your model. For example, you can set the threshold to be 60%:\n",
    "\n",
    "new_pred = (new_pred > 0.6)\n",
    "new_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new threshold still shows that the employee won’t leave the company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving the Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we train our model many times we’ll keep getting different results. The accuracies for each training have a high variance. In order to solve this problem, we’ll use K-fold cross-validation. Usually, K is set to 10. In this technique, the model is trained on the first 9 folds and tested on the last fold. This iteration continues until all folds have been used. Each of the iterations gives its own accuracy. The accuracy of the model becomes the average of all these accuracies.\n",
    "\n",
    "keras enables us to implement K-fold cross-validation via the KerasClassifier wrapper. This wrapper is from scikit-learn cross-validation. We’ll start by importing the cross_val_score cross-validation function and the KerasClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(9, kernel_initializer = \"uniform\", activation = \"relu\", input_dim=18))\n",
    "    classifier.add(Dense(1, kernel_initializer = \"uniform\", activation = \"sigmoid\"))\n",
    "    classifier.compile(optimizer= \"adam\",loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a function that we’ll pass to the KerasClassifier—the function is one of the arguments that the classifier expects. The function is a wrapper of the neural network design that we used earlier. The passed parameters are also similar to the ones used earlier in the tutorial. In the function, we first initialize the classifier using Sequential(), we then use Dense to add the input and output layer. Finally, we compile the classifier and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Convergytics\\AppData\\Local\\Temp\\ipykernel_2496\\128404206.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  classifier = KerasClassifier(build_fn = make_classifier, batch_size=10, nb_epoch=1)\n"
     ]
    }
   ],
   "source": [
    "classifier = KerasClassifier(build_fn = make_classifier, batch_size=10, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KerasClassifier takes three arguments:\n",
    "\n",
    "- build_fn: the function with the neural network design\n",
    "- batch_size: the number of samples to be passed via the network in each iteration\n",
    "- nb_epoch: the number of epochs the network will run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = cross_val_score(estimator = classifier,X = X_train,y = y_train,cv = 10,n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will give us ten accuracies since we have specified the number of folds as 10. Therefore, we assign it to the accuracies variable and later use it to compute the mean accuracy. It takes the following arguments:\n",
    "\n",
    "- estimator: the classifier that you’ve just defined\n",
    "- X: the training set features\n",
    "- y: the value to be predicted in the training set\n",
    "- cv: the number of folds\n",
    "- n_jobs: the number of CPUs to use (specifying it as -1 will make use of all the available CPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.823419040441513"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we have applied the cross-validation, we can compute the mean and variance of the accuracies. \n",
    "\n",
    "mean = accuracies.mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002355765038580735"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To compute the variance of the accuracies, add this code to the next notebook cell:\n",
    "\n",
    "variance = accuracies.var()\n",
    "variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the variance is very low, it means that our model is performing very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve improved our model’s accuracy by using K-Fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Dropout Regularization to Fight Over-Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive models are prone to a problem known as overfitting. This is a scenario whereby the model memorizes the results in the training set and isn’t able to generalize on data that it hasn’t seen. Typically we observe overfitting when we have a very high variance on accuracies. To help fight over-fitting in our model, we will add a layer to our model.\n",
    "\n",
    "In neural networks, dropout regularization is the technique that fights overfitting by adding a Dropout layer in our neural network. It has a rate parameter that indicates the number of neurons that will deactivate at each iteration. The process of deactivating nerurons is usually random. In this case, we specify 0.1 as the rate meaning that 1% of the neurons will deactivate during the training process. The network design remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(9, kernel_initializer = \"uniform\", activation = \"relu\", input_dim=18))\n",
    "classifier.add(Dropout(rate = 0.1))\n",
    "classifier.add(Dense(1, kernel_initializer = \"uniform\", activation = \"sigmoid\"))\n",
    "classifier.compile(optimizer= \"adam\",loss = \"binary_crossentropy\",metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added a Dropout layer between the input and output layer. Having set a dropout rate of 0.1 means that during the training process 15 of the neurons will deactivate so that the classifier doesn’t overfit on the training set. After adding the Dropout and output layers we then compiled the classifier as we have done previously.\n",
    "\n",
    "We worked to fight over-fitting in this step with a Dropout layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search is a technique that we can use to experiment with different model parameters in order to obtain the ones that give us the best accuracy. The technique does this by trying different parameters and returning those that give the best results. We’ll use grid search to search for the best parameters for our deep learning model. This will help in improving model accuracy. scikit-learn provides the GridSearchCV function to enable this functionality. We will now proceed to modify the make_classifier function to try out different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def make_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(9, kernel_initializer = \"uniform\", activation = \"relu\", input_dim=18))\n",
    "    classifier.add(Dense(1, kernel_initializer = \"uniform\", activation = \"sigmoid\"))\n",
    "    classifier.compile(optimizer= optimizer,loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have started by importing GridSearchCV. We have then made changes to the make_classifier function so that we can try different optimizers. We’ve initialized the classifier, added the input and output layer, and then compiled the classifier. Finally, we have returned the classifier so we can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Convergytics\\AppData\\Local\\Temp\\ipykernel_2496\\4215114608.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  classifier = KerasClassifier(build_fn = make_classifier)\n"
     ]
    }
   ],
   "source": [
    "classifier = KerasClassifier(build_fn = make_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve defined the classifier using the KerasClassifier, which expects a function through the build_fn parameter. We have called the KerasClassifier and passed the make_classifier function that we created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now proceed to set a couple of parameters that we wish to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size':[20,35],\n",
    "    'epochs':[2,3],\n",
    "    'optimizer':['adam','rmsprop']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have added different batch sizes, number of epochs, and different types of optimizer functions.\n",
    "\n",
    "For a small dataset like ours, a batch size of between 20–35 is good. For large datasets its important to experiment with larger batch sizes. Using low numbers for the number of epochs ensures that we get results within a short period. However, we can experiment with bigger numbers that will take a while to complete depending on the processing speed of our server. The adam and rmsprop optimizers from keras are a good choice for this type of neural network.\n",
    "\n",
    "Now we’re going to use the different parameters we have defined to search for the best parameters using the GridSearchCV function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=classifier,\n",
    "                           param_grid=params,\n",
    "                           scoring=\"accuracy\",\n",
    "                           cv=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search function expects the following parameters:\n",
    "\n",
    "- estimator: the classifier that you’re using.\n",
    "- param_grid: the set of parameters that you’re going to test.\n",
    "- scoring: the metric you’re using.\n",
    "- cv: the number of folds you’ll test on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "263/263 [==============================] - 1s 938us/step - loss: 0.6012 - accuracy: 0.7506\n",
      "Epoch 2/2\n",
      "263/263 [==============================] - 0s 1ms/step - loss: 0.3955 - accuracy: 0.8457\n",
      "Epoch 1/2\n",
      "263/263 [==============================] - 1s 1ms/step - loss: 0.5989 - accuracy: 0.7636\n",
      "Epoch 2/2\n",
      "263/263 [==============================] - 0s 899us/step - loss: 0.4030 - accuracy: 0.8080\n",
      "Epoch 1/2\n",
      "263/263 [==============================] - 1s 1ms/step - loss: 0.5934 - accuracy: 0.7577\n",
      "Epoch 2/2\n",
      "263/263 [==============================] - 0s 853us/step - loss: 0.4245 - accuracy: 0.7859\n",
      "Epoch 1/2\n",
      "263/263 [==============================] - 1s 1ms/step - loss: 0.5946 - accuracy: 0.7672\n",
      "Epoch 2/2\n",
      "263/263 [==============================] - 0s 895us/step - loss: 0.4272 - accuracy: 0.8084\n",
      "Epoch 1/3\n",
      "263/263 [==============================] - 1s 1ms/step - loss: 0.5724 - accuracy: 0.7594\n",
      "Epoch 2/3\n",
      "263/263 [==============================] - 0s 1ms/step - loss: 0.3905 - accuracy: 0.8295\n",
      "Epoch 3/3\n",
      "263/263 [==============================] - 0s 1ms/step - loss: 0.3151 - accuracy: 0.8765\n",
      "Epoch 1/3\n",
      "263/263 [==============================] - 0s 735us/step - loss: 0.6049 - accuracy: 0.7640\n",
      "Epoch 2/3\n",
      "263/263 [==============================] - 0s 706us/step - loss: 0.3928 - accuracy: 0.8128\n",
      "Epoch 3/3\n",
      "263/263 [==============================] - 0s 772us/step - loss: 0.3210 - accuracy: 0.8358\n",
      "Epoch 1/3\n",
      "263/263 [==============================] - 1s 938us/step - loss: 0.5775 - accuracy: 0.7588\n",
      "Epoch 2/3\n",
      "263/263 [==============================] - 0s 760us/step - loss: 0.4296 - accuracy: 0.7588\n",
      "Epoch 3/3\n",
      "263/263 [==============================] - 0s 770us/step - loss: 0.3886 - accuracy: 0.7588\n",
      "Epoch 1/3\n",
      "263/263 [==============================] - 1s 846us/step - loss: 0.5761 - accuracy: 0.7674\n",
      "Epoch 2/3\n",
      "263/263 [==============================] - 0s 887us/step - loss: 0.4331 - accuracy: 0.7674\n",
      "Epoch 3/3\n",
      "263/263 [==============================] - 0s 877us/step - loss: 0.3859 - accuracy: 0.7674\n",
      "Epoch 1/2\n",
      "150/150 [==============================] - 0s 819us/step - loss: 0.6285 - accuracy: 0.7552\n",
      "Epoch 2/2\n",
      "150/150 [==============================] - 0s 871us/step - loss: 0.4635 - accuracy: 0.7773\n",
      "Epoch 1/2\n",
      "150/150 [==============================] - 0s 726us/step - loss: 0.6286 - accuracy: 0.7659\n",
      "Epoch 2/2\n",
      "150/150 [==============================] - 0s 793us/step - loss: 0.4663 - accuracy: 0.7874\n",
      "Epoch 1/2\n",
      "150/150 [==============================] - 1s 1ms/step - loss: 0.6414 - accuracy: 0.7550\n",
      "Epoch 2/2\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.5141 - accuracy: 0.7588\n",
      "Epoch 1/2\n",
      "150/150 [==============================] - 1s 1ms/step - loss: 0.6375 - accuracy: 0.7611\n",
      "Epoch 2/2\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4978 - accuracy: 0.7686\n",
      "Epoch 1/3\n",
      "150/150 [==============================] - 1s 2ms/step - loss: 0.6375 - accuracy: 0.7514\n",
      "Epoch 2/3\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4752 - accuracy: 0.7880\n",
      "Epoch 3/3\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3838 - accuracy: 0.7971\n",
      "Epoch 1/3\n",
      "150/150 [==============================] - 1s 1ms/step - loss: 0.6315 - accuracy: 0.7636\n",
      "Epoch 2/3\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4718 - accuracy: 0.8057\n",
      "Epoch 3/3\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.3861 - accuracy: 0.8110\n",
      "Epoch 1/3\n",
      "150/150 [==============================] - 1s 2ms/step - loss: 0.6342 - accuracy: 0.7569\n",
      "Epoch 2/3\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.7788\n",
      "Epoch 3/3\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.4119 - accuracy: 0.8099\n",
      "Epoch 1/3\n",
      "150/150 [==============================] - 1s 2ms/step - loss: 0.6207 - accuracy: 0.7672\n",
      "Epoch 2/3\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.7674\n",
      "Epoch 3/3\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4137 - accuracy: 0.8015\n",
      "Epoch 1/3\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.5015 - accuracy: 0.7924\n",
      "Epoch 2/3\n",
      "525/525 [==============================] - 1s 969us/step - loss: 0.2808 - accuracy: 0.9022\n",
      "Epoch 3/3\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.2162 - accuracy: 0.9300\n"
     ]
    }
   ],
   "source": [
    "## Fit the grid search\n",
    "grid_search = grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain the best parameters\n",
    "best_param = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 20, 'epochs': 3, 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can check the best parameters for our model\n",
    "best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output shows that the best batch size is 20, the best number of epochs is 2, and the adam optimizer is the best for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8616985003946329"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can check the best accuracy for your model. The best_accuracy number represents the highest accuracy we obtain \n",
    "# from the best parameters after running the grid search.\n",
    "\n",
    "best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve used GridSearch to figure out the best parameters for our classifier. We have seen that the best batch_size is 20, the best optimizer is the adam optimizer and the best number of epochs is 3. We have also obtained the best accuracy for our classifier as being 86%. We’ve built an employee retention model that is able to predict if an employee stays or leaves with an accuracy of up to 86%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "We’ve used Keras to build an artificial neural network that predicts the probability that an employee will leave a company. To further improve our model, we can try different activation functions or optimizer functions from keras. We could also experiment with a different number of folds, or, even build a model with a different dataset."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06ef7c72ea0fbf6aadbb96241f80366349443c1d206a5cee1b7aaf947634740b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('kamenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
